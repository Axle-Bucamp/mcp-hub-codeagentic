# Fichier de configuration des variables d'environnement
# Copiez ce fichier vers .env et remplissez les valeurs appropriées

# GitHub Personal Access Token (requis pour github-mcp-server)
# Créez un token sur https://github.com/settings/tokens avec les permissions:
# - repo (pour l'accès aux repositories)
# - read:packages (pour l'accès aux images Docker)
# - read:org (pour l'accès aux équipes d'organisation)
GITHUB_PAT=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

# Configuration MCP Proxy (optionnel, valeurs par défaut)
MCP_HOST=0.0.0.0
MCP_PORT=3000
MCP_SSE_PORT=3001

# Configuration Node.js et Python
NODE_ENV=production
PYTHONUNBUFFERED=1

# Configuration Neo4j (pour Graphiti)
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=demodemo

# Configuration des modèles VLLM auto-hébergés
# Endpoint du serveur VLLM
AZURE_OPENAI_ENDPOINT=https://kitty.guidry-cloud.com/v1

# Modèles pour les tâches principales
MODEL_NAME=kitten-kitkat/Qwen3-4B-Thinking-2507
SMALL_MODEL_NAME=kitten-kitkat/Qwen3-4B-Thinking-2507

# Modèle d'embedding (choisissez un modèle compatible avec votre endpoint)
# Options suggérées:
# - sentence-transformers/all-MiniLM-L6-v2 (léger, rapide)
# - sentence-transformers/all-mpnet-base-v2 (plus précis)
# - text-embedding-ada-002 (si compatible avec votre endpoint)
EMBEDDER_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# Configuration Graphiti
SEMAPHORE_LIMIT=10

# Clé API OpenAI (optionnel, utilisé comme fallback)
OPENAI_API_KEY=

